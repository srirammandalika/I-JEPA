{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369d8ae6-2979-4c30-941d-018c1979f674",
   "metadata": {},
   "source": [
    "### Self Supervised Image Representation Prediction with I-JEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce7440-673c-4e07-8d42-319a4f1521ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy, random\n",
    "from einops import rearrange\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————————————————————\n",
    "# 1) Patch Embed + Positional Embedding\n",
    "# ——————————————————————————————————————————————————————————————————————————————\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "        self.grid = img_size // patch_size\n",
    "        self.n_patches = self.grid * self.grid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)                       # [B, D, H/ps, W/ps]\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "        return x                               # [B, N, D]\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————————————————————\n",
    "# 2) I-JEPA model using PyTorch Transformers\n",
    "# ——————————————————————————————————————————————————————————————————————————————\n",
    "class IJEPA_base(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 embed_dim=64,\n",
    "                 enc_depth=6,\n",
    "                 pred_depth=3,\n",
    "                 num_heads=8,\n",
    "                 M=4,\n",
    "                 ema_m=0.996):\n",
    "        super().__init__()\n",
    "        # patch embedding\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size,\n",
    "                                      in_chans=3, embed_dim=embed_dim)\n",
    "        N = self.patch_embed.n_patches\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, N, embed_dim))\n",
    "\n",
    "        # context & target encoders using TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
    "        )\n",
    "        self.ctx_enc = nn.TransformerEncoder(encoder_layer, num_layers=enc_depth)\n",
    "        self.tgt_enc = copy.deepcopy(self.ctx_enc)\n",
    "        for p in self.tgt_enc.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # predictor using TransformerEncoder as decoder proxy\n",
    "        pred_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
    "        )\n",
    "        self.pred = nn.TransformerEncoder(pred_layer, num_layers=pred_depth)\n",
    "\n",
    "        # mask token\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.grid = int(N**0.5)\n",
    "        self.ema_m = ema_m\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_ema(self):\n",
    "        for p_ctx, p_tgt in zip(self.ctx_enc.parameters(),\n",
    "                                self.tgt_enc.parameters()):\n",
    "            p_tgt.data.mul_(self.ema_m) \\\n",
    "                      .add_(p_ctx.data, alpha=1 - self.ema_m)\n",
    "\n",
    "    def sample_block(self, scale_range, ratio_range):\n",
    "        S = random.uniform(*scale_range)\n",
    "        R = random.uniform(*ratio_range)\n",
    "        area = S * self.N\n",
    "        h = int((area * R)**0.5)\n",
    "        w = int((area / R)**0.5)\n",
    "        h = max(1, min(self.grid, h))\n",
    "        w = max(1, min(self.grid, w))\n",
    "        i = random.randint(0, self.grid - h)\n",
    "        j = random.randint(0, self.grid - w)\n",
    "        idxs = [(i+di)*self.grid + (j+dj) for di in range(h) for dj in range(w)]\n",
    "        return idxs\n",
    "\n",
    "    def forward(self, img):\n",
    "        B = img.size(0)\n",
    "        # 1) patch embed + add pos\n",
    "        x = self.patch_embed(img)      # [B,N,D]\n",
    "        x = x + self.pos_emb\n",
    "\n",
    "        # 2) compute target repr\n",
    "        with torch.no_grad():\n",
    "            sy = self.tgt_enc(x)       # [B,N,D]\n",
    "\n",
    "        # 3) sample context blocks and mask\n",
    "        ctx_idxs = [self.sample_block((0.85,1.0),(1.0,1.0)) for _ in range(B)]\n",
    "        x_ctx = x.clone()\n",
    "        for b in range(B):\n",
    "            keep = set(ctx_idxs[b])\n",
    "            mask = [i for i in range(self.N) if i not in keep]\n",
    "            x_ctx[b, mask] = 0\n",
    "        sx = self.ctx_enc(x_ctx)       # [B,N,D]\n",
    "\n",
    "        # 4) sample M target blocks\n",
    "        tgt_idxs = [[self.sample_block((0.15,0.2),(0.75,1.5))\n",
    "                     for _ in range(self.M)] for _ in range(B)]\n",
    "\n",
    "        # 5) predictor\n",
    "        all_preds, all_targs = [], []\n",
    "        for b in range(B):\n",
    "            preds_b, targs_b = [], []\n",
    "            for block in tgt_idxs[b]:\n",
    "                seq = sx[b].clone()     # [N,D]\n",
    "                for idx in block:\n",
    "                    seq[idx] = self.mask_token + self.pos_emb[0, idx]\n",
    "                out = self.pred(seq.unsqueeze(0))  # [1,N,D]\n",
    "                preds_b.append(out[0, block, :])\n",
    "                targs_b.append(sy[b, block, :])\n",
    "            all_preds.append(torch.cat(preds_b, dim=0))\n",
    "            all_targs.append(torch.cat(targs_b, dim=0))\n",
    "\n",
    "        # 6) EMA update\n",
    "        self.update_ema()\n",
    "\n",
    "        return all_preds, all_targs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353960f4-72e1-4d0d-9798-e852c5eb88a2",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb0fca-7bfa-42a8-a307-f6510a3763fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Assume IJEPA_base and PatchEmbed are already defined/imported above\n",
    "\n",
    "# --- 1) Hyperparameters ---\n",
    "img_size      = 32\n",
    "patch_size    = 4\n",
    "embed_dim     = 64\n",
    "enc_depth     = 6\n",
    "pred_depth    = 3\n",
    "num_heads     = 8\n",
    "M             = 4\n",
    "ema_m         = 0.996\n",
    "batch_size    = 64\n",
    "num_epochs    = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# --- 2) Custom PNG dataset ---\n",
    "class PNGFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.paths = sorted(\n",
    "            glob.glob(os.path.join(root, '*.png')),\n",
    "            key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0  # label is unused\n",
    "\n",
    "# --- 3) Prepare dataset & loader ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = PNGFolder(\n",
    "    root='/Users/srirammandalika/Downloads/cifar-10/train',\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,     # avoid pickling with custom class\n",
    "    pin_memory=False   # pin_memory doesn't apply to MPS\n",
    ")\n",
    "\n",
    "# --- 4) Device selection (MPS > CUDA > CPU) ---\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- 5) Initialize model & optimizer ---\n",
    "model = IJEPA_base(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    embed_dim=embed_dim,\n",
    "    enc_depth=enc_depth,\n",
    "    pred_depth=pred_depth,\n",
    "    num_heads=num_heads,\n",
    "    M=M,\n",
    "    ema_m=ema_m\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.ctx_enc.parameters()) + list(model.pred.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "\n",
    "# --- 6) Training loop ---\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(device, non_blocking=False)\n",
    "        preds_list, targs_list = model(imgs)\n",
    "\n",
    "        # compute average L2 loss over all predicted patches\n",
    "        loss_sum = 0.0\n",
    "        patch_count = 0\n",
    "        for preds, targs in zip(preds_list, targs_list):\n",
    "            # ensure targs on same device\n",
    "            targs = targs.to(device, non_blocking=False)\n",
    "            loss_sum += F.mse_loss(preds, targs, reduction='sum')\n",
    "            patch_count += preds.numel()\n",
    "        loss = loss_sum / patch_count\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# --- 7) Quick inference check ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch, _ = next(iter(train_loader))\n",
    "    batch = batch[:8].to(device, non_blocking=False)\n",
    "    preds_list, targs_list = model(batch)\n",
    "    for i, preds in enumerate(preds_list):\n",
    "        print(f\"Sample {i+1}: predicted {preds.shape[0]} patches × {preds.shape[1]}-d embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5aadd-7adc-4a01-96b5-c4b7966b9cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a6b7-a5e1-44ad-be78-19d1102d2a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7697c4c-97a6-4b03-a76c-2969ee0745c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac144b2-f015-4200-b121-3b682afd7193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30225444-6a0f-4942-8dcd-5538e79c3ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26b28-6725-4804-a964-da53d785e4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826731bc-fa00-4ac8-958b-e3228f43894c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a4ee-30f0-4763-b243-276b8ed276b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11b2fa-07e2-416e-a026-5fe8870a2830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e19fc-d9b2-4e88-a029-d494ce0453e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b568f7-d936-46bd-9498-07428fed503e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7da16-b9e7-4a78-bf17-fc22ffd30244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92e3bb-a145-4a3e-ad1f-173f3a3240ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1e543-1f60-41cc-b10a-b638a05c42bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd7c50-7cbc-44ed-a7a2-89150b278ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
